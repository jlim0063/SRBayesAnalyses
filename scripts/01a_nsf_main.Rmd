---
title: "**NSF study Main Data Analyses**"
---

### Load libraries

```{r,echo=FALSE}

## Load Packages

## Data wrangling
library(dplyr)   
library(data.table)
library(tidyr)
library(readxl)      # required for read_excel() method
library(glue)



## Visualisation libraries
library(ggplot2)
library(ggpubr)
library(visreg)
library(ggsignif)    # For significance bars on ggplots
library(ggpubr)

library(egg)         # required for ggarrange (panelling multiple ggplots)

## Modelling
library(lme4)        # required for `Lmer() and glmer()` methods
library(lmerTest)    # required for p-value approximations

library(emmeans)     # required to retrieve least-square estimates, using `lsmeans()` and `emmeans`


library(aod)         # required to call model comparison methods, such as `AIC`, `BIC`
library(car)         # required for Chi-square test of Coefficient Equality `linearHypothesis`
library(MuMIn)       # required for r.squaredGLMM() function
library(performance) # required for check_collinearity function
library(sjPlot)      # required for `tab_model(model)`, which provides a formatted output of the model in APA style.

library(DHARMa)      # required for residual inspection

# Source helper functions script
source("helper-functions.R")


# Miscellaneous libraries not used
# library(piecewiseSEM)


# Turn off scientific notation
options(scipen = 20)

```

### Additional function definition

```{r}

# To retrieve standardised coefficients, if needed (not used)
lm.beta.lmer <- function(mod) {
   b <- fixef(mod)[-1]
   sd.x <- apply(getME(mod,"X")[,-1],2,sd)
   sd.y <- sd(getME(mod,"y"))
   b*sd.x/sd.y
}

```

------------------------------------------------------------------------

# Data Management Pt 1

```{r,echo=FALSE, results = "hide"}

# Read data
nsf.data <- as.data.table(read_excel("../data/exp1_NSF_study_data/data_master.xlsx"))

# Check for duplicates
nsf.data <- nsf.data[duplicated(nsf.data) == FALSE,]

# Convert Condition to factor
nsf.data$Condition <- factor(nsf.data$Condition,
                             levels = c("WR", "TSD", "SR"));
# Convert Session to factor variable
nsf.data$Session <- factor(nsf.data$Session,
                           levels = c(1, 2),
                           labels = c("1st", "2nd"));


```

# Data Management Pt 2

Inspecting the participant IDs show that there are some participants who did not complete both WR and their assigned conditions.

## Participant ID breakdown

```{r, echo=FALSE, results = "hide"}
# Get number of participants that have each respective sleep condition
wr_pts  <- unique(nsf.data[nsf.data$Condition == 'WR', ]$PtID)
tsd_pts <- unique(nsf.data[nsf.data$Condition == 'TSD', ]$PtID)
sr_pts  <- unique(nsf.data[nsf.data$Condition == 'SR', ]$PtID)


# Sort order
wr_pts  <- sort(wr_pts, decreasing = FALSE);
tsd_pts <- sort(tsd_pts, decreasing = FALSE);
sr_pt   <- sort(sr_pts, decreasing = FALSE);

# Print number of participants by condition
length(wr_pts); length(tsd_pts); length(sr_pts)

# Print total number of participants in whole dataset
length(unique(nsf.data$PtID))


setdiff(sr_pts, wr_pts)             # SR participants missing WR data
setdiff(tsd_pts, wr_pts)            # TSD participants missing WR data
setdiff(wr_pts, c(sr_pts, tsd_pts)) # Participants with WR trials data, but missing their respective sleep loss trials data

# need to look at 41 again
```

From inspection of eligibility data: - Data for #41 needs to be dropped completely due to not meeting age criteria. - The following data should be dropped due to idiosyncratic reasons for malperformance on task (e.g. fell asleep during task) - All trials for Participants 13, 22, 45 - TSD trials for Participant 15 - SR trials for Participant 22 - WR trials for Participant 58

```{r, echo = FALSE}

# Store all inegligible participant IDs in one array.
nsf.ineligible_pts <- c(13, 22, 41, 45)

```

## Extract & concatenate participant `KSS` data

```{r, echo=FALSE, results = "hide"}
nsf.temp.KSS <- read.csv("../data/exp1_NSF_study_data/KSS_data.csv")
nsf.temp.KSS <- nsf.temp.KSS[ , c("Subject", "Condition", 'KSS_wr', 'KSS_sl')];

# Create new KSS column in nsf.data
nsf.data$KSS <- NA;
pt_list      <-  unique(nsf.data$PtID);

# Loop through participant ids and extract KSS values from `nsf.kss.data`
for (id in pt_list){
  if(id %in% nsf.temp.KSS$Subject){
    KSS.WR <- nsf.temp.KSS[nsf.temp.KSS$Subject == id, ]$KSS_wr
    KSS.SL <- nsf.temp.KSS[nsf.temp.KSS$Subject == id, ]$KSS_sl
    nsf.data[nsf.data$PtID == id & nsf.data$Condition == "WR", ]$KSS             <- KSS.WR
    nsf.data[nsf.data$PtID == id & nsf.data$Condition %in% c("TSD", "SR"), ]$KSS <- KSS.SL
    print(glue("KSS data for Subject ID {id} added."))
  } else {
    print(glue("No KSS data for Subject ID {id} found."))
  }
}

# Remove placeholder variables from global
rm(pt_list); rm(nsf.temp.KSS)

```

## Extract & conctatenate sex data from original demographic data file, for participants that are missing sex data

```{r}

# Participants with missing demographic information
nsf.missing_sex <- unique(nsf.data[is.na(nsf.data$sex),]$PtID)
nsf.missing_age <- unique(nsf.data[is.na(nsf.data$age),]$PtID)
print(unique(c(nsf.missing_age, nsf.missing_sex)));


# View participants missing KSS data. 
a <- distinct(nsf.data, PtID, Condition,.keep_all = TRUE)
print(a[is.na(a$KSS)==TRUE, 2:3 ])
nsf.missing_kss <- a[is.na(a$KSS)==TRUE, 2:3 ]$PtID

# Add these participants to incomplete pt list
nsf.incomplete_pts <- unique(c(nsf.missing_age, nsf.missing_sex, nsf.missing_kss));

# Remove placeholder variables from global
rm(a)

```

------------------------------------------------------------------------

# Data Management Pt 3

## Fill in missing participant demographic data

These data were retrieved from the participant demographic file.

```{r}

# Fill in missing demographic data if available
nsf.data[nsf.data$PtID == 60, 'age'] <- 19 
nsf.data[nsf.data$PtID == 60, 'sex'] <- 'M'
nsf.data[nsf.data$PtID == 50, 'age'] <- 33 
nsf.data[nsf.data$PtID == 50, 'sex'] <- 'F'
nsf.data[nsf.data$PtID == 3, 'age']  <- 20
nsf.data[nsf.data$PtID == 3, 'sex']  <- 'F'
nsf.data[nsf.data$PtID == 61, 'age'] <- 19
nsf.data[nsf.data$PtID == 61, 'sex'] <- 'M'
nsf.data[nsf.data$PtID == 59, 'age'] <- 23
nsf.data[nsf.data$PtID == 59, 'sex'] <- 'M'
nsf.data[nsf.data$PtID == 41, 'age'] <- 44
nsf.data[nsf.data$PtID == 41, 'sex'] <- 'M'


```

## Drop ineligible participants

```{r, echo = FALSE}
nsf.data <- nsf.data[!(nsf.data$PtID == 22 & nsf.data$Condition == 'SR'), ]  # Drop Participant 22's SR trials**
nsf.data <- nsf.data[!(nsf.data$PtID == 15 & nsf.data$Condition == 'TSD'), ] # Drop Participant 15's TSD trials**
nsf.data <- nsf.data[!(nsf.data$PtID == 58 & nsf.data$Condition == 'WR'), ]  # Drop Participant 58's WR trials

# Drop all participants in ineligible list
nsf.data <- nsf.data[!nsf.data$PtID %in% nsf.ineligible_pts, ]

```

------------------------------------------------------------------------

# Analyses

## KSS model

```{r, echo = FALSE}

# Subset data such that each participant only has 1 KSS entry per condition
nsf.data.KSS <- distinct(nsf.data, PtID, Condition, .keep_all = TRUE)


```

```{r, echo = FALSE}


length(unique(nsf.data.KSS$PtID))
# Check KSS distribution
plot.nsf.KSS.hist   <- ggplot(nsf.data.KSS, aes(x = KSS)) + geom_density()

# Transformations (not used)
nsf.data.KSS$logKSS  <- log(nsf.data.KSS$KSS)
nsf.data.KSS$sqrtKSS <- sqrt(nsf.data.KSS$KSS)
plot.nsf.logKSS.hist <- hist(nsf.data.KSS$logKSS)

# Build the KSS modelconf
m.nsf.KSS <- lmer(KSS ~ 1 + Condition+ (1|PtID), nsf.data.KSS)

# View output
tab_model(m.nsf.KSS)

```

### Normality of Residuals check

```{r}

# Check residuals
sim_output <- simulateResiduals(m.nsf.KSS, plot = F); plotQQunif(sim_output);plotResiduals(sim_output)


```

### Plot KSS model

```{r, echo = FALSE}

nsf.data.KSS.lsmeans <- as_tibble(lsmeans(m.nsf.KSS, specs = "Condition")) %>% 
  slice(match(c("WR", "SR", "TSD"), Condition)) %>%                  # Rearrange such that order is WR, SR, TSD
  mutate(Condition = factor(Condition, levels = c("WR","SR","TSD"))) # Factorize condition such that ggplot follows custom order

# Define plot 

nsf.plot.KSS.lsmeans <- ggplot(nsf.data.KSS.lsmeans, aes(x = Condition, y = lsmean)) +
  geom_col(fill = c("#999DA0", "#777B7E", "#C7C6C1"), colour = "black", linewidth =1) +
  scale_y_continuous(limits = c(0, 8)) +
  labs(x = "Condition", y = "KSS score") + 
  geom_errorbar(aes(ymin = lower.CL, ymax = upper.CL), color = "#222021", width = 0.2, linewidth = 1)+
  theme_pubr() +
  #theme_classic() + 
  theme(aspect.ratio = .8, axis.title = element_text(size = 20),  axis.text = element_text(size = 20), axis.line = element_line(linewidth = 2)) 
  # geom_text(aes(label = round(lsmean, 2)), position = position_dodge(1), vjust = -5, size = 5.5)


# Save plot
ggsave(
  plot = nsf.plot.KSS.lsmeans,
  filename = "../img/nsf/nsf_KSS_score.png",
  bg = "transparent"
  )



# (Only run at end) combined plots
library(cowplot)
combined.plot.KSS.lsmeans <- plot_grid(nsf.plot.KSS.lsmeans, bds.plot.KSS.lsmeans)

ggsave(
  plot = combined.plot.KSS.lsmeans, 
  filename = "../img/combined_KSS_score.png",
  bg = "transparent"
)

```

## Accuracy model


```{r calculate-entropy}
# Create entropy variable
nsf.data.exNB$entropy <- 0
for (row_x in 1:nrow(nsf.data.exNB)){
  prob1 <- nsf.data.exNB[row_x, PosteriorProb]
  prob2 <- 1-prob1
  nsf.data.exNB[row_x, "entropy"] <- calc_entropy(c(prob1, prob2))
}


```

```{r}

# Build accuracy model
m.nsf.ACC <- glmer(
  IsCorrect ~ 1 + Session + entropy + Condition + (1|PtID), 
  family = binomial('logit'),
  data = nsf.data.exNB
)



# Print model summary
summary(m.nsf.ACC)
tab_model(m.nsf.ACC, show.est = T, show.se = T, show.std = T, std.response = F, transform = NULL, digits = 3) 
tab_model(m.nsf.ACC, show.est = T, show.se = T, digits = 3)
 
# Get marginal mean prob of accurate response in each condition
emmeans(m.nsf.ACC, specs ="Condition", type = "response")

```

### Plot accuracy model

```{r, echo = FALSE}

# Code to call least-square marginal means on response scale
nsf.data.ACC <- as_tibble(emmeans(m.nsf.ACC, spec = c( "Condition"), type = "response"))
nsf.data.ACC$Condition <- factor(nsf.data.ACC$Condition, levels = c("WR", "SR", "TSD"))


nsf.plot.ACC <- ggplot(nsf.data.ACC, aes(x = factor(Condition, levels = c("WR","SR","TSD")), y = prob)) + 
  geom_point(size = 3) +
  scale_y_continuous(limits = c(0.5, 1)) +
  labs(x = "Condition", y = "Probability of Accurate Response") +
  scale_color_manual(values = c("#202A44", "#9F1D35")) + 
  geom_errorbar(aes(ymin = nsf.data.ACC$asymp.LCL, ymax = nsf.data.ACC$asymp.UCL), color = '#595959', width = 0.1) + 
  theme_bw()+theme(aspect.ratio = 0.75, axis.title = element_text(size=18), axis.text=element_text(size=15)) ;


# Save plot
# ggsave(
#   plot = nsf.plot.ACC,
#   filename = "img/nsf/NSF_taskAccuracy.png",
#   bg = "transparent"
#   )



```

## Pooled probit model (Decision Weights)

```{r}

# View length of unique IDs
# length(unique(nsf.data.exNB[nsf.data.exNB$Condition == 'TSD',]$PtID))
# length(unique(nsf.data.exNB[nsf.data.exNB$Condition == 'SR', ]$PtID))
# length(unique(nsf.data.exNB[nsf.data.exNB$Condition == 'WR', ]$PtID))
# length(unique(nsf.data.exNB$PtID))


#Build pooled model that excludes NoBrainer trials
m.nsf.exNB <- glmer(PtResp ~ 1 + Condition*LogBaserate + Condition*LogLLA + 
                    (1  |PtID),             
                  family = binomial('probit'),
                  nsf.data.exNB);

# Print model summary
summary(m.nsf.exNB)

```

### Normality of Residuals check

```{r}
# Check residuals
sim_output <- simulateResiduals(m.nsf.exNB, plot = F); plotQQunif(sim_output);plotResiduals(sim_output)


```

### Plot Decision Weights

```{r, echo = FALSE}

# Create DW datatable
condition   <-  c("WR", "WR", "TSD", "TSD", "SR", "SR")
information <- c(rep(c("Evidence", "Base Rate"), 3))
nsf.data.DW <- data.frame(condition, information)

nsf.data.DW$coefficients <- c( 0.71187, 1.38736, 
                              (0.711873 - 0.02729), (1.387346 - 0.0147 ), 
                              (0.711873 - 0.15188), (1.387346 - 0.21893))

nsf.data.DW$ci_lower<- c(0.6392888, 1.2548631,
                         0.5613523, 1.1481704,
                         0.4401318, 0.9502206)


nsf.data.DW$ci_upper <- c(0.7844538490, 1.5198590053,
                           0.8078151152, 1.597176358,
                           0.6798578593, 1.386698082)


nsf.data.DW <- nsf.data.DW %>% setDT()
# Factorise variables
nsf.data.DW[,condition:=factor(nsf.data.DW$condition, levels = c("WR","SR","TSD"))]
nsf.data.DW[,information:= factor(nsf.data.DW$information, levels = c("Base Rate", "Evidence"))]

# Plotting
plot.nsf.DW <- ggplot(nsf.data.DW, aes(x=condition, y=coefficients, shape=information)) + 
  geom_point(size=2, aes(shape = information)) +                          
  scale_y_continuous(limits = c(0, 2)) +                                                          # Sets y axis range      
  labs(x = "Condition", y = "Decision Weight", shape = "Information", title = "Experiment 1") +   # Changes labels (axis titles and legend title)
  
  # scale_shape_manual(values = c(17, 19)) +
  # scale_color_manual(labels = c("Draw Outcome", "Base Rate Odds"), values = c("#696969", "#3E424B"))+
  
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), 
                color = '#595959', width = 0.15, linewidth = .5, position= "identity")+ # Add error bars
  theme_pubr() + theme(aspect.ratio = 1.8)


# Save as png with transparent background
ggsave(plot = plot.nsf.DW,
       filename = "../img/nsf/NSF_decisionWeights.png",
       bg = "transparent");




```


### Plot Relative Decision Weights

```{r rdw-plots}

compute_RDW <- function(b1, b2){
  RDW <- (b1-b2)/(b1+b2)
  return(RDW)
}

for (cond_x in unique(nsf.data.DW$condition)){
  rdw <- nsf.data.DW[condition == cond_x , compute_RDW(coefficients[1], coefficients[2])]
  nsf.data.DW[condition == cond_x , RDW:= rdw]
}

nsf.data.DW$RDW[, RDW:=compute_RDW(coefficients[1], coefficients[2]), by = condition]

nsf.plot.RDW <- ggplot(nsf.data.DW %>% distinct(., condition, .keep_all = T), aes(x = condition, y = RDW)) + 



```

------------------------------------------------------------------------

# RT Analyses



## Prepare data for RT analyses

```{r, echo = FALSE}

# Convert RT to numeric
nsf.data.exNB$RT <- as.numeric(nsf.data.exNB$RT)


# Create Log RT variable
nsf.data.exNB[,logRT:=log(RT)]
nsf.data.exNB[RT > 0,logRT_zscore:=z_score(logRT)]


```

For Experiment 1, trials with 5/6 or 1/6 odds are always paired with number of black balls indicating the 'correct' box is contrary to what the odds suggest. That is,  5/6 odds are always paired with draws of 0, 1, or 2 black balls, and likewise, 1/6 odds trials are always paired with 3, 4, or 5 black balls. Running a model LogRT where LogBaserate is among the covariates would provide the counterintuitive finding that higher LogBaserates are always associated with longer reaction times, when in fact, the conflicting information sources on these trials induce a higher amount of uncertainty. \

To control for the amount of uncertainty induced by conflicting information sources, we first compute the Shannon entropy for each trial which captures the amount of uncertainty on each trial. Entropy values range from 0 to 1, where 1 would indicate maximum uncertainty on a given trial.  Shannon entropy is computed based off the *posterior probability* of each box being the one that was selected, so theoretically, values of entropy also captures some of the information inherent within both Base Rate and Evidence. 



## Build RT models

```{r build-rt-models}
# Subset without noBrainers
nsf.data.exNB     <- nsf.data[nsf.data$NoBrainer == 0, ] # Remove ineligible trials

# Create LogBaserate and LogLLA strength by factorising/taking absolute values
nsf.data.exNB[,LLAstrength:=factor(abs(LogLLA), levels = c(0, 1.38629436111989, 2.77258872223978, 4.15888308335967), labels = c("none", "low", "mid", "high"))]
nsf.data.exNB[,LBRstrength:=factor(abs(LogBaserate), levels = c(0, 0.693147180559947, 1.6094379124341), labels = c("low", "mid", "high"))]


# Build model of LogRT, excluding extreme outliers outside z-score of +- 3.29 (only outliers at z < -3.29 exist).
# Data also consists of 24 trials with anomalous 0 values in RT (logRT would be Inf), so those are dropped from analysis
m.nsf.rt <- lmer(logRT~ 1 + Condition*LBRstrength + Condition*LLAstrength + entropy + Session +  (1|PtID), 
                 data = nsf.data.exNB[logRT_zscore>-3.29 & RT > 0])


# Since all interaction terms were n.s., rebuild model dropping interactions as all n.s.
m.nsf.rt <- lmer(logRT ~ 1 + Condition + LBRstrength + LLAstrength + entropy + Session +(1 |PtID), 
                                  data = nsf.data.exNB[logRT_zscore>-3.29 & RT > 0])

# Inspect model summary
sjPlot::tab_model(m.nsf.rt)
summary(m.nsf.rt)

```

END
